# -*- coding: utf-8 -*-
"""
The Advanced Cloud RAG Pipeline: Egyptian Edition.

This script executes a robust, multi-step RAG pipeline using a full suite
of managed Azure services. It's designed to test Arabic language handling
on a production-grade cloud stack and serve as a direct comparison to the
local implementation.
"""

import time
import numpy as np
import logging
from dotenv import load_dotenv

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

from minichain.text_splitters.token_splitter import TokenTextSplitter
# Import the necessary components from our Mini-Chain framework
from minichain.core.types import Document
from minichain.text_splitters.implementations import RecursiveCharacterTextSplitter
from minichain.embeddings.azure import AzureOpenAIEmbeddings
from minichain.chat_models.azure import AzureOpenAIChatModel
from minichain.vector_stores.azure_ai_search import AzureAISearchVectorStore
from minichain.prompts.implementations import PromptTemplate

# --- Setup for robust logging ---

def setup_logging():
    """Sets up a comprehensive logging system for the pipeline."""
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
        
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('rag_pipeline_cloud_arabic.log', encoding='utf-8', mode='w'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

# --- Helper Functions ---

def print_header(title: str):
    """Prints a formatted header to the console."""
    print("\n" + "#" * 70)
    print(f"## {title.upper()} ".ljust(68) + "##")
    print("#" * 70)

def clean_arabic_text(text: str) -> str:
    """A simple function to normalize Arabic text from LLM outputs."""
    return ' '.join(text.split()).strip()

def main():
    """
    Executes the advanced, cloud-native, and robust Egyptian RAG pipeline.
    """
    logger = setup_logging()
    
    try:
        # --- PRE-FLIGHT CHECK ---
        print_header("Ø®Ø·ÙˆØ© Ø§Ø³ØªÙƒØ´Ø§ÙÙŠØ©: ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø³Ø­Ø§Ø¨Ø© Azure")
        load_dotenv()
        
        required_vars = [
            "AZURE_OPENAI_ENDPOINT", "AZURE_OPENAI_API_KEY", "AZURE_OPENAI_DEPLOYMENT_NAME",
            "AZURE_OPENAI_ENDPOINT_EMBEDDINGS", "AZURE_OPENAI_EMBEDDINGS_API_KEY", "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME",
            "AZURE_AI_SEARCH_ENDPOINT", "AZURE_AI_SEARCH_ADMIN_KEY"
        ]
        if not all(os.getenv(var) for var in required_vars):
            logger.error("âŒ ERROR: Missing one or more required Azure environment variables.")
            print("âŒ ERROR: Missing required Azure credentials in .env file.")
            return

        logger.info("âœ… All Azure credentials found. Proceeding with the cloud pipeline.")

        # --- STEP 1: INITIALIZE CLOUD COMPONENTS ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ (Azure)")
        azure_chat_model = AzureOpenAIChatModel(deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"), temperature=0.7) # type: ignore
        azure_embeddings = AzureOpenAIEmbeddings(deployment_name=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")) # type: ignore
        # text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
        text_splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=30)
        
        index_name = f"minichain-egyptian-demo-{int(time.time())}"
        vector_store = AzureAISearchVectorStore(embeddings=azure_embeddings, index_name=index_name)
        logger.info("âœ… All Azure-native components initialized successfully.")

        # --- STEP 2: THE TEAM'S ORIGIN STORY ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ø¨Ø±Ø¯ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø¯Ø³Ø© (Ù‚ØµØ© Ø§Ù„ÙØ±ÙŠÙ‚)")
        team_story = """
        ÙÙ„Ø³ÙØ© 'Mini-Chain' Ù…ØªØ¬Ø°Ø±Ø© ÙÙŠ Ø®Ø¨Ø±Ø§Øª ÙØ±ÙŠÙ‚Ù‡ Ø§Ù„ÙØ±ÙŠØ¯Ø©. Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙŠØ·ÙˆØ±Ù‡ ÙØ§Ø¯ÙŠØŒ
        Ù…ØªØ¯Ø±Ø¨ Ù…Ø¬ØªÙ‡Ø¯ ÙˆØ´Ù‚ÙŠØ§Ù†ØŒ Ù„ÙƒÙ† Ø±ÙˆØ­Ù‡ Ø¨ØªØªØ´ÙƒÙ„ Ù…Ù† Ø§Ù„Ø£Ø³Ø§Ø·ÙŠØ± Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ±Ø§Ø¬Ø¹ÙˆØ§ Ø´ØºÙ„Ù‡.

        Ø£Ù…ÙŠÙ†ØŒ ÙˆØ§Ø­Ø¯ Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ÙŠÙ†ØŒ Ø¹Ù†Ø¯Ù‡ Ø®Ø¨Ø±Ø© Ù…Ù† Ø²Ù…Ù† ØªØ§Ù†ÙŠ Ù‚Ø¨Ù„ Ø¹ØµØ± Ø§Ù„Ù€ LLMs.
        Ø¯Ù‡ ÙŠØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¯ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ†ØŒ ÙƒØ§Ù† Ø¨ÙŠØ¨Ù†ÙŠ Ù…ÙˆØ¯ÙŠÙ„Ø² Computer Vision Ø³Ù†Ø© 2017.
        ÙÙŠ Ø§Ù„Ø£ÙŠØ§Ù… Ø¯ÙŠØŒ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙƒØ§Ù† Ù…Ø¹Ù†Ø§Ù‡ Ø¥Ù†Ùƒ "Ø¨ØªÙ†Ø­Øª ÙÙŠ Ø§Ù„ØµØ®Ø± Ø¹Ù„Ù‰ Stack Overflow" Ù„Ø³Ø§Ø¹Ø§ØªØŒ
        ÙˆØ¨ØªØªØ®Ø§Ù†Ù‚ Ù…Ø¹ ÙˆØ­ÙˆØ´ Ù‚Ø¯ÙŠÙ…Ø© Ø²ÙŠ Caffe Ùˆ Theano. Ù…ÙˆØ§ÙÙ‚ØªÙ‡ Ù…Ø¹Ù†Ø§Ù‡Ø§ Ø¥Ù† Ø§Ù„ÙƒÙˆØ¯ Ø¯Ù‡ Ù…Ø´ Ø¨Ø³ Ø°ÙƒÙŠØŒ Ù„Ø£ Ø¯Ù‡ Ù‚ÙˆÙŠ ÙˆÙ…ØªÙŠÙ†.

        Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªØ§Ù†ÙŠØŒ ÙŠØ­ÙŠÙ‰ØŒ Ù‡Ùˆ ØªØ¬Ø³ÙŠØ¯ Ù„Ø±ÙˆØ­ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ÙØªÙˆØ­Ø© ÙˆØ§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ³ØªÙ….
        ÙƒÙ…Ø³ØªØ®Ø¯Ù… Ù‚Ø¯ÙŠÙ… Ù„Ù€ Arch LinuxØŒ Ù‡Ùˆ ÙØ§Ù‡Ù… Ù‚ÙˆØ© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø³ÙŠØ· ÙˆØ§Ù„Ù…ØªØ¸Ø¨Ø· ØµØ­ØŒ Ø§Ù„Ù„ÙŠ ÙƒÙ„ Ø­ØªØ© ÙÙŠÙ‡
        Ù…Ø®ØªØ§Ø±Ø© Ù„Ù‡Ø¯Ù. ØªØ£Ø«ÙŠØ±Ù‡ Ù‡Ùˆ Ø§Ù„Ø³Ø¨Ø¨ Ø¥Ù† Mini-Chain Ø¨ÙŠØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø²Ø§Ø¦Ø¯ ÙˆØ¨ÙŠØ­ØªØ±Ù… ÙÙ„Ø³ÙØ© Ù„ÙŠÙ†ÙƒØ³:
        "Ø§Ø¹Ù…Ù„ Ø­Ø§Ø¬Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ Ø¨Ø³ Ø§Ø¹Ù…Ù„Ù‡Ø§ ØµØ­ Ø§Ù„ØµØ­".
        """
        print(f"Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ù‚ØµØ© Ø§Ù„ÙØ±ÙŠÙ‚):\n---\n{team_story.strip()}\n---")
        documents = text_splitter.create_documents([team_story], [{"source": "asateer_el_fareeq.md"}])
        logger.info(f"âœ… The story has been parsed into {len(documents)} chunks.")

        # --- STEP 3: INDEXING THE STORY ON AZURE ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© (Azure AI)")
        vector_store.add_documents(documents)
        logger.info("Waiting for Azure AI Search to index documents...")
        time.sleep(5)
        logger.info("âœ… Story indexed in Azure AI Search.")

        # --- ADVANCED STEP 4: QUERY TRANSFORMATION ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø¹Ù†Ø§Ù† Ù„Ø¹Ù‚Ù„ Azure (ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„)")
        original_question = "Ø§Ø­ÙƒÙŠÙ„ÙŠ Ø¹Ù† Ø´Ù‚Ù‰ Ø£Ù…ÙŠÙ† ÙˆØªØ¹Ø¨ Ø§Ù„Ø³Ù†ÙŠÙ† ÙÙŠ Ø§Ù„Ø´ØºÙ„Ø§Ù†Ø© Ø¯ÙŠ."
        print(f"Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ØµÙ„ÙŠ: '{original_question}'")
        
        query_creation_prompt = PromptTemplate(
            template="Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨Ø­Ø« ÙÙ‡Ù„ÙˆÙŠ. Ù…Ù‡Ù…ØªÙƒ ØªØ­ÙˆÙ‘Ù„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø­Ø« Ù…Ø®ØªØµØ± ÙˆØºÙ†ÙŠ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.\n\nØ³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}\n\nØµÙŠØºØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©:"
        )
        optimized_query = azure_chat_model.invoke(query_creation_prompt.format(question=original_question))
        optimized_query = clean_arabic_text(optimized_query)
        print(f"Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¹Ù‡ Azure: '{optimized_query.strip()}'")
        logger.info(f"Azure AI refined the search query to: '{optimized_query.strip()}'")

        # --- STEP 5: RETRIEVAL FROM AZURE AI SEARCH ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© ÙÙŠ Ø³Ø­Ø§Ø¨Ø© Azure")
        retrieved_docs = vector_store.similarity_search(optimized_query, k=1)
        logger.info(f"ğŸ” Found {len(retrieved_docs)} relevant chunks from Azure AI Search.")
        print(f"ğŸ” Azure AI Search Ù„Ù‚Ù‰ Ø£ÙƒØªØ± Ø­ØªØ© Ø°Ø§ÙƒØ±Ø© Ù„ÙŠÙ‡Ø§ Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹:")
        for doc in retrieved_docs:
            print(f"  -> '{doc.page_content}'")

        # --- ADVANCED STEP 6: PERSONALITY-DRIVEN GENERATION ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 6: ØµÙŠØ§ØºØ© Ø§Ù„Ø­ÙƒØ§ÙŠØ© (Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´Ø®ØµÙŠØ© Ù…ØµØ±ÙŠØ© Ù…Ù† Azure)")
        context_string = "\n---\n".join([doc.page_content for doc in retrieved_docs])
        storyteller_prompt = PromptTemplate(
            template="""Ø£Ù†Øª Ø­ÙƒÙˆØ§ØªÙŠ Ù…ØµØ±ÙŠ Ø£Ø³Ø·ÙˆØ±ÙŠ ÙˆØ¯Ù…Ùƒ Ø®ÙÙŠÙ. Ù…Ù‡Ù…ØªÙƒ ØªØ¬Ø§ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø´Ø¹Ø¨ÙŠ ÙˆØ¯Ø±Ø§Ù…ÙŠ.
### Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ùƒ:
{context}
### Ø§Ù„Ø³Ø¤Ø§Ù„:
{original_question}
### Ø§Ø­ÙƒÙŠ Ø§Ù„Ø­ÙƒØ§ÙŠØ© ÙŠØ§ ÙÙ†Ø§Ù†:"""
        )
        final_prompt = storyteller_prompt.format(context=context_string, original_question=original_question)
        print("âœ¨ Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ù€ promptØŒ Ø¬Ø§Ù‡Ø²Ø© Ø¹Ø´Ø§Ù† ØªØ±ÙˆØ­ Ù„Ù€ Azure:\n---")
        print(final_prompt.strip())
        print("---")
        
        final_answer = azure_chat_model.invoke(final_prompt)
        final_answer = clean_arabic_text(final_answer)
        logger.info(f"Final answer generated by Azure, length: {len(final_answer)}")
        
        # --- FINAL OUTPUT ---
        print("\n" + "*" * 70)
        print(" Ø­ÙƒØ§ÙŠØ© Ù…Ù† Ù‚Ù„Ø¨ Ø³Ø­Ø§Ø¨Ø© Azure ".center(70, " "))
        print("*" * 70)
        print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {original_question}")
        print("\nØ±Ø¯ Ø§Ù„Ø­ÙƒÙˆØ§ØªÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ:")
        print(final_answer)
        print("*" * 70)

        # --- STEP 7: CLEANUP ---
        print_header("Ø§Ù„Ø®Ø·ÙˆØ© 7: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©")
        try:
            vector_store.index_client.delete_index(index_name)
            logger.info(f"âœ… Successfully deleted temporary index '{index_name}'.")
        except Exception as e:
            logger.error(f"âš ï¸ Failed to delete index '{index_name}'. Please delete manually.", exc_info=True)

    except Exception as e:
        logger.error("âŒ An unexpected error occurred in the main cloud pipeline.", exc_info=True)
        print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

if __name__ == "__main__":
    main()