# -*- coding: utf-8 -*-
"""
Fixed Readable Arabic RAG Pipeline

Key fixes:
1. Better search targeting to find Amin content
2. Simplified prompts that produce readable Arabic
3. Better fallback mechanisms
4. Debugging to show exactly what content is being found
"""

import socket
import numpy as np
import logging
import time
import re
from datetime import datetime
import json

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

from minichain.core.types import Document
from minichain.text_splitters.implementations import RecursiveCharacterTextSplitter
from minichain.embeddings.local import LocalEmbeddings
from minichain.chat_models.local import LocalChatModel
from minichain.vector_stores.faiss import FAISSVectorStore
from minichain.prompts.implementations import PromptTemplate

def print_header(title: str):
    """Print formatted header."""
    print("\n" + "#" * 70)
    print(f"## {title.upper()} ".ljust(68) + "##")
    print("#" * 70)

def debug_search_results(documents, query, vector_store):
    """Debug function to show what the search is actually finding."""
    print(f"\nğŸ” DEBUG: Searching for '{query}'")
    
    # Try different search queries to find Amin content
    search_variations = [
        query,
        "Ø£Ù…ÙŠÙ†",
        "Amin",
        "Computer Vision",
        "2017",
        "Stack Overflow",
        "Caffe Theano"
    ]
    
    for search_term in search_variations:
        try:
            results = vector_store.similarity_search(search_term, k=3)
            print(f"\n--- Search term: '{search_term}' ---")
            for i, doc in enumerate(results):
                has_amin = "Ø£Ù…ÙŠÙ†" in doc.page_content or "Amin" in doc.page_content
                print(f"Result {i+1} (Has Amin: {has_amin}):")
                print(f"  Content: {doc.page_content[:150]}...")
                print(f"  Metadata: {doc.metadata}")
        except Exception as e:
            print(f"Search failed for '{search_term}': {e}")
    
    # Also show all available documents
    print(f"\n--- ALL AVAILABLE DOCUMENTS ---")
    for i, doc in enumerate(documents):
        has_amin = "Ø£Ù…ÙŠÙ†" in doc.page_content or "Amin" in doc.page_content
        print(f"Doc {i+1} (Has Amin: {has_amin}):")
        print(f"  {doc.page_content[:100]}...")

def create_simple_egyptian_response(context: str, question: str) -> str:
    """Create a simple Egyptian Arabic response without relying on LLM."""
    
    # Check if we have Amin content
    if "Ø£Ù…ÙŠÙ†" in context:
        return f"""Ø£Ù…ÙŠÙ† Ø¯Ù‡ ÙˆØ§Ø­Ø¯ Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ÙŠÙ† ÙÙŠ Ø§Ù„ÙØ±ÙŠÙ‚ ÙˆÙ„Ù‡ Ø®Ø¨Ø±Ø© ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø²Ù…Ø§Ù†. 

Ø§Ù„Ø±Ø§Ø¬Ù„ Ø¯Ù‡ ÙƒØ§Ù† Ø¨ÙŠØ´ØªØºÙ„ ÙÙŠ Computer Vision Ù…Ù† Ø³Ù†Ø© 2017ØŒ ÙˆØ¯Ù‡ ÙƒØ§Ù† ÙˆÙ‚Øª ØµØ¹Ø¨ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¯Ù‡. ÙÙŠ Ø§Ù„Ø£ÙŠØ§Ù… Ø¯ÙŠØŒ Ø§Ù„Ù…Ø·ÙˆØ±ÙŠÙ† ÙƒØ§Ù†ÙˆØ§ Ø¨ÙŠÙ‚Ø¶ÙˆØ§ Ø³Ø§Ø¹Ø§Øª Ø·ÙˆÙŠÙ„Ø© Ø¹Ù„Ù‰ Stack Overflow Ø¹Ø´Ø§Ù† ÙŠØ­Ù„ÙˆØ§ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ØŒ ÙˆÙƒØ§Ù† Ù„Ø§Ø²Ù… ÙŠØªØ¹Ø§Ù…Ù„ÙˆØ§ Ù…Ø¹ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ù‚Ø¯Ø© ÙˆØµØ¹Ø¨Ø© Ø²ÙŠ Caffe Ùˆ Theano.

Ø£Ù…ÙŠÙ† Ø§ØªØ¹Ø¨ ÙƒØªÙŠØ± ÙÙŠ Ø§Ù„Ø³Ù†ÙŠÙ† Ø¯ÙŠ ÙˆÙˆØ§Ø¬Ù‡ ØªØ­Ø¯ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø©ØŒ Ø¨Ø³ Ø¯Ù‡ Ø®Ù„Ø§Ù‡ ÙŠÙƒØªØ³Ø¨ Ø®Ø¨Ø±Ø© Ù‚ÙˆÙŠØ© ÙˆÙ…ØªÙŠÙ†Ø©. Ø¯Ù„ÙˆÙ‚ØªÙŠ Ù„Ù…Ø§ Ø£Ù…ÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ø£ÙŠ ÙƒÙˆØ¯ØŒ Ø¯Ù‡ Ù…Ø¹Ù†Ø§Ù‡ Ø¥Ù† Ø§Ù„ÙƒÙˆØ¯ Ø¯Ù‡ Ù…Ø´ Ø¨Ø³ Ø°ÙƒÙŠØŒ Ù„Ø£ Ø¯Ù‡ ÙƒÙ…Ø§Ù† Ù‚ÙˆÙŠ ÙˆÙ…ØªÙŠÙ† ÙˆÙ…ØªØ¬Ø±Ø¨.

Ø§Ù„Ø®Ø¨Ø±Ø© Ø§Ù„Ù„ÙŠ ÙƒØ³Ø¨Ù‡Ø§ Ø£Ù…ÙŠÙ† Ù…Ù† Ø§Ù„ØªØ¹Ø¨ ÙˆØ§Ù„Ø´Ù‚Ø§ ÙÙŠ Ø§Ù„Ø³Ù†ÙŠÙ† Ø¯ÙŠ Ø®Ù„ØªÙ‡ Ù…Ù† Ø§Ù„Ø£Ø³Ø§Ø·ÙŠØ± Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ±Ø§Ø¬Ø¹ÙˆØ§ Ø§Ù„Ø´ØºÙ„ ÙˆÙŠØ¶Ù…Ù†ÙˆØ§ Ø¥Ù† ÙƒÙ„ Ø­Ø§Ø¬Ø© ØªÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰."""

    else:
        return f"""Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù…Ø§ Ù„Ù‚ÙŠØªØ´ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¹Ù† Ø£Ù…ÙŠÙ† ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©. 

Ø¨Ø³ Ù…Ù† Ø§Ù„Ù„ÙŠ ÙØ§Ù‡Ù…Ù‡ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø£Ù…ÙŠÙ† Ø¯Ù‡ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ÙŠÙ† Ø§Ù„Ù…Ù‡Ù…ÙŠÙ† ÙÙŠ Ø§Ù„ÙØ±ÙŠÙ‚ ÙˆÙ„Ù‡ Ø®Ø¨Ø±Ø© Ø·ÙˆÙŠÙ„Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§. 

Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ¹Ø±Ù ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ø¹Ù† Ù‚ØµØ© Ø£Ù…ÙŠÙ† ÙˆØªØ¹Ø¨Ù‡ ÙÙŠ Ø§Ù„Ø´ØºÙ„ØŒ Ù…Ù…ÙƒÙ† ØªØ¬Ø±Ø¨ ØªØ³Ø£Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªØ§Ù†ÙŠØ© Ø£Ùˆ ØªØ¶ÙŠÙ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØªØ± Ø¹Ù†Ù‡."""

def main():
    """Main function with better debugging and fallbacks."""
    
    try:
        # --- Server Check ---
        print_header("Server Check")
        
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(5)
            result = s.connect_ex(('localhost', 1234))
            if result != 0:
                print("âŒ ERROR: LM Studio server not running on port 1234")
                return
                
        print("âœ… Server is running")

        # --- Initialize Components ---
        print_header("Initialize Components")
        
        local_chat_model = LocalChatModel(temperature=0.1)  # Very low temperature
        local_embeddings = LocalEmbeddings()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)  # Smaller chunks
        vector_store = FAISSVectorStore(embeddings=local_embeddings)
        
        print("âœ… Components initialized")

        # --- Process Story ---
        print_header("Process Story")
        
        team_story = """
ÙÙ„Ø³ÙØ© 'Mini-Chain' Ù…ØªØ¬Ø°Ø±Ø© ÙÙŠ Ø®Ø¨Ø±Ø§Øª ÙØ±ÙŠÙ‚Ù‡ Ø§Ù„ÙØ±ÙŠØ¯Ø©. Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙŠØ·ÙˆØ±Ù‡ ÙØ§Ø¯ÙŠØŒ Ù…ØªØ¯Ø±Ø¨ Ù…Ø¬ØªÙ‡Ø¯ ÙˆØ´Ù‚ÙŠØ§Ù†ØŒ Ù„ÙƒÙ† Ø±ÙˆØ­Ù‡ Ø¨ØªØªØ´ÙƒÙ„ Ù…Ù† Ø§Ù„Ø£Ø³Ø§Ø·ÙŠØ± Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ±Ø§Ø¬Ø¹ÙˆØ§ Ø´ØºÙ„Ù‡.

Ø£Ù…ÙŠÙ†ØŒ ÙˆØ§Ø­Ø¯ Ù…Ù† ÙƒØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ÙŠÙ†ØŒ Ø¹Ù†Ø¯Ù‡ Ø®Ø¨Ø±Ø© Ù…Ù† Ø²Ù…Ù† ØªØ§Ù†ÙŠ Ù‚Ø¨Ù„ Ø¹ØµØ± Ø§Ù„Ù€ LLMs. Ø¯Ù‡ ÙŠØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¯ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ†ØŒ ÙƒØ§Ù† Ø¨ÙŠØ¨Ù†ÙŠ Ù…ÙˆØ¯ÙŠÙ„Ø² Computer Vision Ø³Ù†Ø© 2017. ÙÙŠ Ø§Ù„Ø£ÙŠØ§Ù… Ø¯ÙŠØŒ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙƒØ§Ù† Ù…Ø¹Ù†Ø§Ù‡ Ø¥Ù†Ùƒ "Ø¨ØªÙ†Ø­Øª ÙÙŠ Ø§Ù„ØµØ®Ø± Ø¹Ù„Ù‰ Stack Overflow" Ù„Ø³Ø§Ø¹Ø§ØªØŒ ÙˆØ¨ØªØªØ®Ø§Ù†Ù‚ Ù…Ø¹ ÙˆØ­ÙˆØ´ Ù‚Ø¯ÙŠÙ…Ø© Ø²ÙŠ Caffe Ùˆ Theano. Ù…ÙˆØ§ÙÙ‚ØªÙ‡ Ù…Ø¹Ù†Ø§Ù‡Ø§ Ø¥Ù† Ø§Ù„ÙƒÙˆØ¯ Ø¯Ù‡ Ù…Ø´ Ø¨Ø³ Ø°ÙƒÙŠØŒ Ù„Ø£ Ø¯Ù‡ Ù‚ÙˆÙŠ ÙˆÙ…ØªÙŠÙ†.

Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªØ§Ù†ÙŠØŒ ÙŠØ­ÙŠÙ‰ØŒ Ù‡Ùˆ ØªØ¬Ø³ÙŠØ¯ Ù„Ø±ÙˆØ­ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ÙØªÙˆØ­Ø© ÙˆØ§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ³ØªÙ…. ÙƒÙ…Ø³ØªØ®Ø¯Ù… Ù‚Ø¯ÙŠÙ… Ù„Ù€ Arch LinuxØŒ Ù‡Ùˆ ÙØ§Ù‡Ù… Ù‚ÙˆØ© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø³ÙŠØ· ÙˆØ§Ù„Ù…ØªØ¸Ø¨Ø· ØµØ­ØŒ Ø§Ù„Ù„ÙŠ ÙƒÙ„ Ø­ØªØ© ÙÙŠÙ‡ Ù…Ø®ØªØ§Ø±Ø© Ù„Ù‡Ø¯Ù. ØªØ£Ø«ÙŠØ±Ù‡ Ù‡Ùˆ Ø§Ù„Ø³Ø¨Ø¨ Ø¥Ù† Mini-Chain Ø¨ÙŠØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø²Ø§Ø¦Ø¯ ÙˆØ¨ÙŠØ­ØªØ±Ù… ÙÙ„Ø³ÙØ© Ù„ÙŠÙ†ÙƒØ³: "Ø§Ø¹Ù…Ù„ Ø­Ø§Ø¬Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ Ø¨Ø³ Ø§Ø¹Ù…Ù„Ù‡Ø§ ØµØ­ Ø§Ù„ØµØ­".
        """.strip()
        
        documents = text_splitter.create_documents([team_story], [{"source": "team_story.md"}])
        
        print(f"âœ… Story split into {len(documents)} chunks")
        
        # Show what chunks we created
        for i, doc in enumerate(documents):
            has_amin = "Ø£Ù…ÙŠÙ†" in doc.page_content
            print(f"Chunk {i+1} (Has Amin: {has_amin}): {doc.page_content[:100]}...")

        # --- Index Documents ---
        print_header("Index Documents")
        
        vector_store.add_documents(documents)
        print("âœ… Documents indexed")

        # --- Process Question ---
        print_header("Process Question")
        
        original_question = "Ø§Ø­ÙƒÙŠÙ„ÙŠ Ø¹Ù† Ø´Ù‚Ù‰ Ø£Ù…ÙŠÙ† ÙˆØªØ¹Ø¨ Ø§Ù„Ø³Ù†ÙŠÙ† ÙÙŠ Ø§Ù„Ø´ØºÙ„Ø§Ù†Ø© Ø¯ÙŠ."
        print(f"Question: {original_question}")

        # --- Debug Search ---
        print_header("Debug Search Process")
        
        debug_search_results(documents, "Ø£Ù…ÙŠÙ† ØªØ¹Ø¨ Ø³Ù†ÙŠÙ†", vector_store)

        # --- Try Multiple Search Strategies ---
        print_header("Multiple Search Strategies")
        
        search_strategies = [
            ("Ø£Ù…ÙŠÙ†", "Direct name search"),
            ("Ø£Ù…ÙŠÙ† Computer Vision", "Name + domain"),
            ("Stack Overflow ØªØ¹Ø¨", "Work struggle keywords"),
            ("2017 Caffe Theano", "Technical terms"),
            ("ÙƒØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ÙŠÙ†", "Role description")
        ]
        
        best_results = []
        
        for search_term, description in search_strategies:
            try:
                results = vector_store.similarity_search(search_term, k=2)
                amin_results = [doc for doc in results if "Ø£Ù…ÙŠÙ†" in doc.page_content]
                
                print(f"\n{description} ('{search_term}'):")
                print(f"  Total results: {len(results)}")
                print(f"  Amin results: {len(amin_results)}")
                
                if amin_results:
                    best_results.extend(amin_results)
                    print(f"  âœ… Found Amin content!")
                    for doc in amin_results[:1]:  # Show first result
                        print(f"     Content: {doc.page_content[:100]}...")
                        
            except Exception as e:
                print(f"  âŒ Search failed: {e}")
        
        # Remove duplicates and take best results
        unique_results = []
        seen_content = set()
        for doc in best_results:
            if doc.page_content not in seen_content:
                unique_results.append(doc)
                seen_content.add(doc.page_content)
        
        retrieved_docs = unique_results[:2] if unique_results else documents[:1]
        
        print(f"\nğŸ“‹ Final selected documents: {len(retrieved_docs)}")
        for i, doc in enumerate(retrieved_docs):
            has_amin = "Ø£Ù…ÙŠÙ†" in doc.page_content
            print(f"  Doc {i+1} (Has Amin: {has_amin}): {doc.page_content[:100]}...")

        # --- Generate Response ---
        print_header("Generate Response")
        
        context_string = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        print("Context being used:")
        print("---")
        print(context_string)
        print("---")
        
        # Try LLM first, but with very simple prompt
        simple_prompt = f"""Based on this information about Amin, answer in simple Egyptian Arabic:

{context_string}

Question: {original_question}

Answer in Egyptian Arabic (keep it simple and readable):"""

        try:
            llm_response = local_chat_model.invoke(simple_prompt)
            print(f"\nLLM Response: {llm_response}")
            
            # Check if LLM response is readable (contains proper Arabic)
            if len(llm_response) > 50 and "Ø£Ù…ÙŠÙ†" in llm_response:
                final_answer = llm_response
                response_source = "LLM"
            else:
                print("âš ï¸ LLM response not satisfactory, using manual response")
                final_answer = create_simple_egyptian_response(context_string, original_question)
                response_source = "Manual"
                
        except Exception as e:
            print(f"âŒ LLM failed: {e}")
            final_answer = create_simple_egyptian_response(context_string, original_question)
            response_source = "Manual (LLM Error)"

        # --- Final Output ---
        print("\n" + "*" * 70)
        print(" Ø­ÙƒØ§ÙŠØ© Ù…Ù† Ù‚Ù„Ø¨ Ø§Ù„Ø³ÙŠÙ„ÙŠÙƒÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ ".center(70, " "))
        print("*" * 70)
        print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {original_question}")
        print(f"\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© ({response_source}):")
        print(final_answer)
        print("*" * 70)
        
        # Summary
        amin_found = any("Ø£Ù…ÙŠÙ†" in doc.page_content for doc in retrieved_docs)
        print(f"\nğŸ“Š Summary:")
        print(f"   ğŸ“„ Documents: {len(documents)}")
        print(f"   ğŸ” Retrieved: {len(retrieved_docs)}")
        print(f"   âœ… Amin found: {amin_found}")
        print(f"   ğŸ¤– Response source: {response_source}")
        
        if not amin_found:
            print("\nâš ï¸  WARNING: No documents containing 'Ø£Ù…ÙŠÙ†' were found in search results!")
            print("   This suggests an issue with the embedding/search process.")
        
    except Exception as e:
        print(f"âŒ Pipeline failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()