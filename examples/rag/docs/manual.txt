"""
About the Mini-Chain Framework**

    Mini-Chain is a micro-framework for building applications with Large Language Models.
    Its written by fady mohamed on his internship at ist networks as a required task by eng amin.
    Its core principle is transparency and modularity. Unlike larger frameworks that
    can feel like a "black box," Mini-Chain provides clean, single-purpose classes
    for each stage of a RAG pipeline.
    Core Components:
    - Chat Models (`LocalChatModel`, `AzureOpenAIChatModel`):** These classes provide a
      unified interface to interact with different LLM providers. The `invoke` method is
      the primary way to get a response.
    - Embeddings (`LocalEmbeddings`, `AzureOpenAIEmbeddings`):** These are used to convert
      text into numerical vectors. They have `embed_documents` for lists of texts and
      `embed_query` for single texts.
    - Memory (`FAISSVectorStore`, `AzureAISearchVectorStore`):** These components store
      the vectorized text chunks. `FAISSVectorStore` is a fast, local, in-memory option,
      perfect for development. It supports saving and loading from disk.
      `AzureAISearchVectorStore` is a scalable cloud solution for production.
      The main method is `similarity_search`.
    - Text Splitters (`TokenTextSplitter`, `RecursiveCharacterTextSplitter`):** These
      are used to break large documents into smaller, manageable chunks before embedding.
      `TokenTextSplitter` is recommended as it aligns with how models process tokens.
    - Prompts (`PromptTemplate`, `FewShotPromptTemplate`, `ChatPromptTemplate`):**
      Powered by a Jinja2 engine, these classes allow for dynamic and complex prompt
      creation.
    - Output Parsers (`PydanticOutputParser`): This powerful tool ensures that the
      LLM's output is not just a string, but a validated, type-safe Pydantic object,
      making the output reliable and easy to use in downstream logic.
"""